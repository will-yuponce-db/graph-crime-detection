# Databricks Asset Bundle Configuration
# Cross-Jurisdictional Investigative Analytics Demo
#
# Deploy with:
#   databricks bundle deploy -t dev
#   databricks bundle deploy -t prod
#
# Run pipeline:
#   databricks bundle run investigative_analytics_pipeline -t dev

bundle:
  name: investigative-analytics

# Variable definitions for environment-specific values
variables:
  catalog:
    description: Unity Catalog name for pipeline tables
    default: pubsec_geo_law
  schema:
    description: Schema name within the catalog
    default: demo
  notification_email:
    description: Email for job notifications
    default: "scott.johnson@databricks.com"
  lakebase_instance_name:
    description: Name of the Lakebase Postgres instance
    default: investigative-analytics-pg
  lakebase_database:
    description: Postgres database name inside the Lakebase instance
    default: investigative_analytics
  lakebase_catalog:
    description: UC database catalog name for the Lakebase instance
    default: investigative_analytics_pg

# Default workspace configuration
# Authenticate with: databricks auth login --host https://fe-vm-industry-solutions-buildathon.cloud.databricks.com
# root_path auto-resolves to the deploying user's workspace directory
workspace:
  host: https://e2-demo-field-eng.cloud.databricks.com
  root_path: /Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.name}/my-envs/${bundle.target}

# Shared resource definitions
resources:
  # Lakebase Postgres infrastructure
  database_instances:
    lakebase_instance:
      name: ${var.lakebase_instance_name}
      capacity: CU_1

  database_catalogs:
    lakebase_catalog:
      database_instance_name: ${resources.database_instances.lakebase_instance.name}
      name: ${var.lakebase_catalog}
      database_name: ${var.lakebase_database}
      create_database_if_not_exists: true

  pipelines:
    investigative_analytics_pipeline:
      name: "Investigative Analytics - ${bundle.target}"
      catalog: ${var.catalog}
      target: ${var.schema}
      libraries:
        - file:
            path: ./data_generation_dlt.py
      serverless: true
      continuous: false
      photon: true
      channel: PREVIEW
      configuration:
        "spark.databricks.delta.preview.enabled": "true"
        "pipelines.numStreamRetryAttempts": "5"

  jobs:
    demo_pipeline_job:
      name: "Investigative Analytics Pipeline - ${bundle.target}"
      description: "Generates demo data, validates output, and syncs to Lakebase Postgres"
      environments:
        - environment_key: serverless_env
          spec:
            client: "1"
      tasks:
        - task_key: run_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.investigative_analytics_pipeline.id}
            full_refresh: true
        - task_key: validate_data
          depends_on:
            - task_key: run_pipeline
          notebook_task:
            notebook_path: ./validate_data_generation.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
          environment_key: serverless_env
        - task_key: sync_to_lakebase
          depends_on:
            - task_key: validate_data
          notebook_task:
            notebook_path: ./trigger_lakebase_sync.py
            base_parameters:
              lakebase_catalog: ${var.lakebase_catalog}
              source_schema: ${var.schema}
          environment_key: serverless_env
      email_notifications:
        on_failure:
          - ${var.notification_email}
      tags:
        project: investigative-analytics
        environment: ${bundle.target}

    # One-time job to create synced tables in Lakebase
    # Run manually: databricks bundle run lakebase_setup_job -t dev
    lakebase_setup_job:
      name: "Lakebase Sync Setup - ${bundle.target}"
      description: "One-time job to create synced tables from UC to Lakebase Postgres"
      environments:
        - environment_key: serverless_env
          spec:
            client: "1"
      tasks:
        - task_key: create_synced_tables
          notebook_task:
            notebook_path: ./setup_lakebase_sync.py
            base_parameters:
              source_catalog: ${var.catalog}
              source_schema: ${var.schema}
              lakebase_catalog: ${var.lakebase_catalog}
              lakebase_database: ${var.lakebase_database}
              lakebase_instance: ${var.lakebase_instance_name}
          environment_key: serverless_env
      email_notifications:
        on_failure:
          - ${var.notification_email}
      tags:
        project: investigative-analytics
        environment: ${bundle.target}
        purpose: lakebase-setup

# Environment-specific targets
targets:
  # Development environment
  dev:
    mode: development
    default: true
    variables:
      catalog: pubsec_geo_law
      schema: demo
    resources:
      pipelines:
        investigative_analytics_pipeline:
          development: true

  # Staging environment
  staging:
    mode: development
    variables:
      catalog: pubsec_geo_law
      schema: demo
    resources:
      pipelines:
        investigative_analytics_pipeline:
          development: true

  # Production environment
  prod:
    mode: production
    variables:
      catalog: investigative_analytics
      schema: demo
    resources:
      pipelines:
        investigative_analytics_pipeline:
          development: false
          channel: CURRENT
          notifications:
            - email_recipients:
                - ${var.notification_email}
              alerts:
                - on-failure
                - on-flow-failure
